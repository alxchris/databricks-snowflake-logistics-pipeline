# databricks-snowflake-logistics-pipeline
ðŸš€ **Databricks â†’ Snowflake Logistics Analytics Pipeline
Production-Style Cross-Platform Data Engineering Case Study**

This project demonstrates a production-style data engineering pipeline built using:
â€¢	**Databricks (Delta Lake, Unity Catalog)
â€¢	Snowflake (Cloud Data Warehouse)**
â€¢	Parquet-based data exchange
â€¢	Incremental upserts (MERGE)
â€¢	Data quality enforcement

The pipeline simulates a logistics company processing shipment, customer, and operational event data to produce business-ready KPIs.
________________________________________
# ðŸ— Architecture Overview

## High-Level Flow

Raw Data Generation  
â†“  
Databricks Bronze (Raw Delta Tables)  
â†“  
Databricks Silver (Cleaned + Deduplicated Data)  
â†“  
Databricks Gold (Business KPI Tables)  
â†“  
Export as Parquet Files  
â†“  
Snowflake Internal Stages  
â†“  
Structured Warehouse Tables

Databricks handles transformation and data quality logic.  
Snowflake serves as the analytical warehouse layer.

________________________________________

## ðŸ”· Architecture Diagram

![Databricks Snowflake Architecture](assets/databricks-snowflake-architecture.png)

________________________________________
# ðŸ§± Databricks Layering

## ðŸ¥‰ Bronze Layer
- Raw ingestion
- Schema-on-read
- Stored in Delta format

## ðŸ¥ˆ Silver Layer
- Deduplication using Delta MERGE
- Business rule enforcement:
  - Remove negative shipment weights
  - Validate origin/destination ports
- Ensures 1 record per `shipment_id`

## ðŸ¥‡ Gold Layer
Curated KPI tables:
- `gold_customer_monthly_kpis`
- `gold_lane_kpis`
- `gold_latest_shipment_event_status`

  ðŸ” Incremental Processing (MERGE Logic)
  Delta Lake MERGE used for idempotent upserts:

  Python:
  delta_target.alias("t") \
  .merge(
      ship_latest.alias("s"),
      "t.shipment_id = s.shipment_id"
  ) \
  .whenMatchedUpdateAll() \
  .whenNotMatchedInsertAll() \
  .execute()
  
This simulates production-grade incremental processing.
________________________________________
ðŸ“¦ Snowflake Integration
Due to serverless compute constraints, Gold datasets were:
1.Exported as Parquet from Databricks
2.Uploaded to Snowflake internal stages
3.Loaded using CTAS with structured column casting

Example:
SQL:
CREATE OR REPLACE TABLE GOLD_LANE_KPIS AS
SELECT
  $1:origin_port::STRING AS origin_port,
  $1:dest_port::STRING AS dest_port,
  $1:shipments::NUMBER AS shipments
FROM @CASE02_LANE_STAGE
(FILE_FORMAT => 'CASE02_PARQUET');
________________________________________
ðŸ“Š**Warehouse Output Tables**
**GOLD_LANE_KPIS**
Lane performance metrics:
â€¢	Shipments
â€¢	Avg transit hours
â€¢	On-time rate
â€¢	Total spend

**GOLD_CUSTOMER_MONTHLY_KPIS**
Customer-level KPIs:
â€¢	Monthly shipment volume
â€¢	Average shipment cost
â€¢	Delivery performance

**GOLD_LATEST_SHIPMENT_EVENT_STATUS**
Operational snapshot:
â€¢	Latest shipment event
â€¢	Event timestamp
â€¢	Location
â€¢	Notes
________________________________________
ðŸ›¡ Data Quality Controls
â€¢	Negative weights filtered out
â€¢	Port dimension validation
â€¢	Deduplication using grouping + MERGE
________________________________________
ðŸ“ˆ Final Row Counts
| Table                             | Rows   |
| --------------------------------- | ------ |
| GOLD_LANE_KPIS                    | 26,922 |
| GOLD_CUSTOMER_MONTHLY_KPIS        | 23,328 |
| GOLD_LATEST_SHIPMENT_EVENT_STATUS | 73,438 |
________________________________________
ðŸ”§ Technologies Used
â€¢	Apache Spark (PySpark)
â€¢	Delta Lake
â€¢	Unity Catalog
â€¢	Snowflake
â€¢	Parquet
â€¢	SQL
â€¢	Data Warehousing concepts
________________________________________
ðŸŽ¯ What This Demonstrates
â€¢	Multi-layer medallion architecture
â€¢	Delta Lake MERGE logic
â€¢	Data quality enforcement
â€¢	Cross-platform integration (Databricks â†’ Snowflake)
â€¢	Warehouse-ready structured modeling
________________________________________
ðŸ“Œ Author
Alexander Christodoulou
Senior Database Engineer transitioning into modern Data Engineering & Analytics.

