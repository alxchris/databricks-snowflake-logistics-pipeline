![Databricks](https://img.shields.io/badge/Databricks-FF3621?logo=databricks&logoColor=white)
![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?logo=snowflake&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta_Lake-003B57?logo=apache-spark&logoColor=white)

# databricks-snowflake-logistics-pipeline
üöÄ **Databricks ‚Üí Snowflake Logistics Analytics Pipeline
Production-Style Cross-Platform Data Engineering Case Study**

This project demonstrates a production-style data engineering pipeline built using:
‚Ä¢	**Databricks (Delta Lake, Unity Catalog)
‚Ä¢	Snowflake (Cloud Data Warehouse)**
‚Ä¢	Parquet-based data exchange
‚Ä¢	Incremental upserts (MERGE)
‚Ä¢	Data quality enforcement

The pipeline simulates a logistics company processing shipment, customer, and operational event data to produce business-ready KPIs.
________________________________________
# üèó Architecture Overview

## High-Level Flow

Raw Data Generation  
‚Üì  
Databricks Bronze (Raw Delta Tables)  
‚Üì  
Databricks Silver (Cleaned + Deduplicated Data)  
‚Üì  
Databricks Gold (Business KPI Tables)  
‚Üì  
Export as Parquet Files  
‚Üì  
Snowflake Internal Stages  
‚Üì  
Structured Warehouse Tables

Databricks handles transformation and data quality logic.  
Snowflake serves as the analytical warehouse layer.

________________________________________

## üî∑ Architecture Diagram
<p align="center">
  <img src="assets/databricks-snowflake-architecture.png" width="700"/>
</p>

________________________________________
# üß± Databricks Layering

## ü•â Bronze Layer
- Raw ingestion
- Schema-on-read
- Stored in Delta format

## ü•à Silver Layer
- Deduplication using Delta MERGE
- Business rule enforcement:
  - Remove negative shipment weights
  - Validate origin/destination ports
- Ensures 1 record per `shipment_id`

## ü•á Gold Layer
Curated KPI tables:
- `gold_customer_monthly_kpis`
- `gold_lane_kpis`
- `gold_latest_shipment_event_status`

  üîÅ Incremental Processing (MERGE Logic)
  Delta Lake MERGE used for idempotent upserts:

  Python:
  delta_target.alias("t") \
  .merge(
      ship_latest.alias("s"),
      "t.shipment_id = s.shipment_id"
  ) \
  .whenMatchedUpdateAll() \
  .whenNotMatchedInsertAll() \
  .execute()
  
This simulates production-grade incremental processing.
________________________________________
üì¶ Snowflake Integration
Due to serverless compute constraints, Gold datasets were:
1.Exported as Parquet from Databricks
2.Uploaded to Snowflake internal stages
3.Loaded using CTAS with structured column casting

Example:
SQL:
CREATE OR REPLACE TABLE GOLD_LANE_KPIS AS
SELECT
  $1:origin_port::STRING AS origin_port,
  $1:dest_port::STRING AS dest_port,
  $1:shipments::NUMBER AS shipments
FROM @CASE02_LANE_STAGE
(FILE_FORMAT => 'CASE02_PARQUET');
________________________________________
üìä**Warehouse Output Tables**
**GOLD_LANE_KPIS**
Lane performance metrics:
‚Ä¢	Shipments
‚Ä¢	Avg transit hours
‚Ä¢	On-time rate
‚Ä¢	Total spend

**GOLD_CUSTOMER_MONTHLY_KPIS**
Customer-level KPIs:
‚Ä¢	Monthly shipment volume
‚Ä¢	Average shipment cost
‚Ä¢	Delivery performance

**GOLD_LATEST_SHIPMENT_EVENT_STATUS**
Operational snapshot:
‚Ä¢	Latest shipment event
‚Ä¢	Event timestamp
‚Ä¢	Location
‚Ä¢	Notes
________________________________________
üõ° Data Quality Controls
‚Ä¢	Negative weights filtered out
‚Ä¢	Port dimension validation
‚Ä¢	Deduplication using grouping + MERGE
________________________________________
üìà Final Row Counts
| Table                             | Rows   |
| --------------------------------- | ------ |
| GOLD_LANE_KPIS                    | 26,922 |
| GOLD_CUSTOMER_MONTHLY_KPIS        | 23,328 |
| GOLD_LATEST_SHIPMENT_EVENT_STATUS | 73,438 |
________________________________________
üîß Technologies Used
‚Ä¢	Apache Spark (PySpark)
‚Ä¢	Delta Lake
‚Ä¢	Unity Catalog
‚Ä¢	Snowflake
‚Ä¢	Parquet
‚Ä¢	SQL
‚Ä¢	Data Warehousing concepts
________________________________________
üéØ What This Demonstrates
‚Ä¢	Multi-layer medallion architecture
‚Ä¢	Delta Lake MERGE logic
‚Ä¢	Data quality enforcement
‚Ä¢	Cross-platform integration (Databricks ‚Üí Snowflake)
‚Ä¢	Warehouse-ready structured modeling
________________________________________
üìå Author
Alexander Christodoulou
Senior Database Engineer transitioning into modern Data Engineering & Analytics.

