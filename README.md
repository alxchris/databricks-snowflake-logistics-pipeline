# databricks-snowflake-logistics-pipeline
ðŸš€ **Databricks â†’ Snowflake Logistics Analytics Pipeline
Production-Style Cross-Platform Data Engineering Case Study**

This project demonstrates a production-style data engineering pipeline built using:
â€¢	**Databricks (Delta Lake, Unity Catalog)
â€¢	Snowflake (Cloud Data Warehouse)**
â€¢	Parquet-based data exchange
â€¢	Incremental upserts (MERGE)
â€¢	Data quality enforcement

The pipeline simulates a logistics company processing shipment, customer, and operational event data to produce business-ready KPIs.
________________________________________
ðŸ— Architecture Overview

Raw Data Generation
        â†“
Databricks Bronze (Raw Delta)
        â†“
Databricks Silver (Cleaned + Deduplicated)
        â†“
Databricks Gold (Business KPIs)
        â†“
Export as Parquet
        â†“
Snowflake Internal Stages
        â†“
Structured Warehouse Tables

Databricks handles transformation and data quality logic.
Snowflake serves as the analytical warehouse layer.
________________________________________
ðŸ§± Databricks Layering
**Bronze Layer**
  â€¢	Raw ingestion
  â€¢	Schema-on-read
  â€¢	Stored in Delta format
**Silver Layer**
  â€¢	Deduplication using Delta MERGE
  â€¢	Business rule enforcement:
      o	Remove negative shipment weights
      o	Validate origin/destination ports
  â€¢	Ensures 1 record per shipment_id
**Gold Layer**
Curated KPI tables:
  â€¢	gold_customer_monthly_kpis
  â€¢	gold_lane_kpis
  â€¢	gold_latest_shipment_event_status

  ðŸ” Incremental Processing (MERGE Logic)
  Delta Lake MERGE used for idempotent upserts:

  Python:
  delta_target.alias("t") \
  .merge(
      ship_latest.alias("s"),
      "t.shipment_id = s.shipment_id"
  ) \
  .whenMatchedUpdateAll() \
  .whenNotMatchedInsertAll() \
  .execute()
  
This simulates production-grade incremental processing.
________________________________________
ðŸ“¦ Snowflake Integration
Due to serverless compute constraints, Gold datasets were:
1.Exported as Parquet from Databricks
2.Uploaded to Snowflake internal stages
3.Loaded using CTAS with structured column casting

Example:
SQL:
CREATE OR REPLACE TABLE GOLD_LANE_KPIS AS
SELECT
  $1:origin_port::STRING AS origin_port,
  $1:dest_port::STRING AS dest_port,
  $1:shipments::NUMBER AS shipments
FROM @CASE02_LANE_STAGE
(FILE_FORMAT => 'CASE02_PARQUET');
________________________________________
ðŸ“Š**Warehouse Output Tables**
**GOLD_LANE_KPIS**
Lane performance metrics:
â€¢	Shipments
â€¢	Avg transit hours
â€¢	On-time rate
â€¢	Total spend

**GOLD_CUSTOMER_MONTHLY_KPIS**
Customer-level KPIs:
â€¢	Monthly shipment volume
â€¢	Average shipment cost
â€¢	Delivery performance

**GOLD_LATEST_SHIPMENT_EVENT_STATUS**
Operational snapshot:
â€¢	Latest shipment event
â€¢	Event timestamp
â€¢	Location
â€¢	Notes
________________________________________
ðŸ›¡ Data Quality Controls
â€¢	Negative weights filtered out
â€¢	Port dimension validation
â€¢	Deduplication using grouping + MERGE
________________________________________
ðŸ“ˆ Final Row Counts
| Table                             | Rows   |
| --------------------------------- | ------ |
| GOLD_LANE_KPIS                    | 26,922 |
| GOLD_CUSTOMER_MONTHLY_KPIS        | 23,328 |
| GOLD_LATEST_SHIPMENT_EVENT_STATUS | 73,438 |
________________________________________
ðŸ”§ Technologies Used
â€¢	Apache Spark (PySpark)
â€¢	Delta Lake
â€¢	Unity Catalog
â€¢	Snowflake
â€¢	Parquet
â€¢	SQL
â€¢	Data Warehousing concepts
________________________________________
ðŸŽ¯ What This Demonstrates
â€¢	Multi-layer medallion architecture
â€¢	Delta Lake MERGE logic
â€¢	Data quality enforcement
â€¢	Cross-platform integration (Databricks â†’ Snowflake)
â€¢	Warehouse-ready structured modeling
________________________________________
ðŸ“Œ Author
Alexander Christodoulou
Senior Database Engineer transitioning into modern Data Engineering & Analytics.

